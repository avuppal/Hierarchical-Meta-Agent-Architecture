version: '3.8'

services:
  hma-service:
    build: .
    container_name: hma-orchestrator
    ports:
      - "8080:8080"
    environment:
      - VLLM_API_BASE=http://vllm-worker:8000/v1
      - VLLM_API_KEY=EMPTY
      - VLLM_MODEL_NAME=facebook/opt-125m
      - HMA_API_BASE=http://vllm-worker:8000/v1
      - HMA_API_KEY=EMPTY
      - HMA_MODEL_NAME=facebook/opt-125m
      - MAX_CONCURRENT_WORKERS=32
    depends_on:
      - vllm-worker
    volumes:
      - ./hma_benchmark_logs.csv:/app/hma_benchmark_logs.csv
      - ./hma_semantic_cache_db:/app/hma_semantic_cache_db
      - ./data:/app/data # Mount local data folder for Librarian RAG

  vllm-worker:
    image: vllm/vllm-openai:latest
    container_name: vllm-inference
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 
              capabilities: [gpu]
    ports:
      - "8000:8000"
    # PERFORMANCE TUNING:
    # 1. Quantization: Use '--quantization awq' with AWQ models (e.g. TheBloke/Llama-2-70B-AWQ) to save VRAM.
    # 2. Speculative Decoding: Add '--speculative-model [small-model]' for 2x speedup.
    command: >
      --model facebook/opt-125m
      --gpu-memory-utilization 0.90
      --max-model-len 2048
      --enable-prefix-caching
      --disable-log-stats
      --enforce-eager
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ipc: host 
